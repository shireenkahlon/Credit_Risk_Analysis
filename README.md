# Credit_Risk_Analysis
This project will use supervised machine learning processes to check for fraudulent credit card applications.

## Overview of the analysis
The purpose of this project is to build a supervised machine learning model which will predict strong and weak applications for loans. The machine learning model is designed to create training and testing datasets from the original dataset. In addition, it will use different techniques, such as random oversampling, SMOTE, and SMOTEENN to balance the imbalances in the data between the two target classes. The end result of this project will assist my client, Fast Lending, in predicting which applications they should accept or reject for loans. For reference, 0 represents the loans that are the “bad applications” or the application that would be recommended for rejection. On the other hand, 1 represents the “good applications” or the loan applications that would be accepted. 

## Results
* The first model shows random oversampling on the original data. This model will increase the number of data points in the minority target class to match the number in the majority class. The results, as shown below, display the accuracy score to be 64%; the accuracy score is the percentage of predictions that are correct compared to the y testing values. However, 64% is a low percentage to rely upon for incoming data. The next statistic, precision, looks at how accurate the predictions are for each target class. The average for the precision is high but as we can see in the results below, the model often predicts class 0 values wrong. The recall looks at the actual numbers versus the predictions. The recall for class 0 is relatively high, at 0.71; a recall of 0.71 is saying that less than 3 out of 4 of the actual results were predicted correctly. On the other hand, the recall for class 1 is barely over half — 0.57; this shows us that a little over half of the actual results for class 1 were predicted correctly. Now that recall and precision have been explained, we can examine the remainder of the results from the different balancing imbalanced data methods.
![random_oversampling](https://github.com/shireenkahlon/Credit_Risk_Analysis/blob/main/Screenshot%20Images/random_oversampling.png)

* The next model to examine is the SMOTE model which synthetically adds data points to the minority class to balance the minority and majority target classes. The accuracy score in the SMOTE model is 65% — which is a moderately high accuracy score. The precision and recall, as seen below, shows us that the model made some accurate predictions but not enough to rely on when separating the bad applications.
![SMOTE](https://github.com/shireenkahlon/Credit_Risk_Analysis/blob/main/Screenshot%20Images/SMOTE.png)

* The following model is the undersampling method; this method resamples data by removing data points from the majority group to level it with the minority group. The model will be trained with this resampled data and tested with our original data. From this resampling method and model, we can see that the accuracy score is 54% — lower than the oversampling and SMOTE models. In addition, the recall is relatively low considering the actual results for class 1 did not match with the predictions made. The 0.1 precision score shows us that the model was not able to accurately predict the results for class 0. The model predicted the class 1 values almost perfectly.
![undersampling](https://github.com/shireenkahlon/Credit_Risk_Analysis/blob/main/Screenshot%20Images/undersampling.png)

* The model shown below is the SMOTEENN model which combines undersampling and oversampling. The accuracy score is 64% — this score is moderately high. The recall for class 1 shows us that the actual results and the predicted results did not match up. As seen below and in the previous models, the SMOTEENN model was not able to accurately predict class 0’s but did well in predicting class 1’s.
![SMOTEENN](https://github.com/shireenkahlon/Credit_Risk_Analysis/blob/main/Screenshot%20Images/SMOTEENN.png)

* The following classification report shows the results of the Balanced Random Forest Classifier which takes the minority target class and adds subsets of the majority class. This process is recreated for each decision tree. The results, as shown below, have a higher accuracy score at 79% and the precision and recall are both higher than the other models; this signifies that the model was able to accurately predict the target classes.
![balanced_random_forest_classifier](https://github.com/shireenkahlon/Credit_Risk_Analysis/blob/main/Screenshot%20Images/balanced_random_forest_classifier.png)

* The final result is the Easy Ensemble AdaBoost Classifier. The Easy Ensemble AdaBoost Classifier is similar to the Balanced Random Forest Classifier except this is a booster so each tree learns from the previous tree. The results show us that this method is very reliable with a 93% accuracy score and a very high recall and precision rates. The model only wrongly predicted 8 of the class 0 values and a small percentage of the class 1 values. 
![easy_ensemble_adaboost_classifier](https://github.com/shireenkahlon/Credit_Risk_Analysis/blob/main/Screenshot%20Images/easy_ensemble_adaboost_classifier.png)

## Summary
To summarize the results, the balance models that were used to fix the imbalances in the target classes did not reliably predict the results. All three had an accuracy score between 54-65% and low precision/recall rates. On the other hand, the ensemble classifiers had high accuracy scores and precision/recall rates with the Easy Ensemble AdaBoost Classifier having a 93% accuracy score. In order to get more adequate results, we may have to test the ensemble models on further data to ensure accuracy. For an immediate recommendation, I would propose the Easy Ensemble AdaBoost Classifier, as the statistics of this model show us the accurate predictions it is able to make.
 
